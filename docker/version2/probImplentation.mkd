<meta http-equiv='Content-Type' content='text/html; charset=utf-8' />
<style>
pre{background:#F8F8FF; border:black dashed 1px; padding:6px}
</style>

# Index


* [ Mise en contexte ](#context) 
* [ Création des images et conteneurs ](#conteneurPropre) 
       * [ Un conteneur pour une taches ](#ConteneurUnetache) 
       * [ Création du conteneur de BD centraliser.](#ConteneurCentraliser) 
       * [ Docker n'ajoute pas d'overheard, oui MAIS ...](#noOverHead) 
       * [ Utilisation du registry](#UseRegistry) 
       * [ Création d'un conteneur avec une image "moins belle"](#ConteneurCommité) 
* [ Orchestration des conteneurs](#orchestration) 
       * [ Organisation d'un changement de variable ou volume](#ChangeVarVol) 
       * [ Organisation des fichiers et volumes](#FileStructure) 
* [ Mise en place de réseau dédié à docker](#NetworkStructure) 
       * [ Redirection des ports](#Redirect_port) 
           * [ Détection des ip d'origine](#Redirect_port_srcip) 
           * [ Conteneur http utilisation des port 80/443](#Redictect_port_Http) 
           * [ Le défis des adresse IP dynamique](#ChallangeDynIP) 
       * [ Utilisation de conteneur exclusif au VPN](#ConteneurVPNonly) 
* [ Problème lors d'un disque dur remplie](#ProbHDFull) 
       * [ Limitation d'espace par conteneur](#SpaceLimitConteneur) 
* [ Optimisation de Dockers](#OptDocker) 
       * [ Assignation d'un Volume Groupe (LVM)](#OptDockerFS) 
       * [ Monitoring du système Docker](#MonitoringDck) 

# <a name="context" /> Mise en contexte 

Nous avons vu précédemment les grandes lignes de l'utilisation de docker, c'est les grands principes de l'utilisation du système après libre à vous de réaliser votre propre recette. L'informatique et plus particulièrement l'opensource c'est un peu comme les recettes de cuisine, il y a une recette qui est une base mais si vous aimez plus ingrédient rien vous empêches d'en mettre plus :P !

Par contre le gros problèmes de l'ensemble des documentations est qu'ils donnent rarement des trucs et astuce ou relater des problème lors de l'implémentation :-/. Je vais donne vous fournir de l'information sur des problèmes que j'ai rencontré ainsi que les solutions mise en place afin d'au moins vous orienter si un problème similaire vous arrive.
L'expérience couvre 1 an d'utilisation plus une mise en production pour un serveur personnel !


# <a name="conteneurPropre" /> Création des images et conteneurs 

Bien entendu en premier lieu il y a la création d'image, personnellement l'ensemble de mes images furent réalisé à 85 % dans le train sur un petit portable 64bits (__Lenovo T430__) on est loin d'un ultra portable. Les 15 % restant est sur mon poste de travail à la maison , le gros avantage est que ce fut un travail itératif, continue, le tous conservé avec **git** !

J'ai identifié les applications que je désirai avoir et analyser s'il y avait des conteneurs officiel disponible !

Voici les officiels que j'ai pu utiliser :

* **httpd**
* **mysql**
* **debian**
* **ubuntu**
* **ngnix**
* **gitlab/gitlab-ce:latest**

Comme vous pouvez le constater ceci est principalement des images de base , sauf **gitlab** qui est considéré comme non officiel sur le site [http://hub.docker.com](http://hub.docker.com), mais qui est créé par **gitlab**.

Bien entendu mon site à plus de service que __MySQL__ , __gitlab__ ou __ngnix__ ... 

J'ai donc du réalisé soit des **Dockerfile** ou utiliser des images dit public !! 
J'ai voulu mettre en place le service __tmate__ qui permet de partagé un **Shell** avec __tmux__ en __RO__ ou __RW__. Bon finalement ce n'est pas en place :P, j'ai tenté de faire le __Dockerfile__ mais les priorités ont changé en cours de réalisation. Si nous regardons sur le site de [http://hub.docker.com](http://hub.docker.com) vous trouverez une images disponible :

* [https://hub.docker.com/r/dakue/tmate-slave/~/dockerfile/](https://hub.docker.com/r/dakue/tmate-slave/~/dockerfile/) 

Je n'aime pas du tous cette configuration, voici pourquoi :

* La récupération d'un artefact est réalisé sur **circle-artifacts.com**, personnellement je ne connais pas ce qui ne me met pas en confiance !

Je n'ai pas d'autre exemple en tête mais l'important du propos est de bien valider les images que vous prenez , je vous dirai même dans le doute prenez comme référence un __Dockerfile__ et créez le votre.

Je vous invite **FORTEMENT** à utilisé un contrôleur de révision afin d'avoir une trace de l'évolution de vos images afin de pouvoir revenir en arrière ou vous inspirér des configurations réalisé.

## <a name="ConteneurUnetache" /> Un conteneur pour une taches 

Bon voilà , nous avons vue la théorie les principes le mode de fonctionnement et les principes. Maintenant on va voir la pratique, qui malheureusement diverge de la théorie. 

Prenons le conteneur officiel de **gitlab** [https://hub.docker.com/r/gitlab/gitlab-ce/](https://hub.docker.com/r/gitlab/gitlab-ce/), cette image est très bien créer et fonctionne à merveille. De plus comme ceci est la version officiel fournit par l'entreprise **gitlab** nous avons l'avantage d'avoir une version régulièrement mise à jour nous assurant le maintient de l'application autant au niveau __bug__ que patch de sécurité !

Malheureusement l'image ne suis pas le principe UNE image , UNE taches , l'images contient plusieurs application dans l'images formant un tous cohérent, mais les puristes diront qu'elles n'est pas BELLE.

L'image **gitlab** est composé de :

* __ngnix__ :  pour l'accès web ainsi que les commit via le protocole __http__
* __unicorn__ : serveur web en **ruby** pour faire le traitement des requêtes, permet d'utiliser l'ensemble des fonctions de __ruby__
* __postgress__ : Base de donnée pour stocker l'information
* __sidekiq__ : un système de traitement en arrière plan des requêtes permettant de traiter des jobs en __batch__
* __sshd__ : Permet de réaliser des commit dans les dépôts avec le protocole ssh

Bon on est loin d'une image = un processus, suivant le principe nous aurions du avoir 5 conteneurs qui interagisse ensemble pour fournir le service, mais bon de vous à moins j'avais pas envie de faire mon propre __Dockerfile__ et surtout de devoir le maintenir dans le temps. Comme je ne suis pas un puriste pour tous , sauf le logiciel libre , j'ai pris le conteneur et il fonctionne à merveille !!!

Je parle ici du conteneur __gitlab__, mais j'ai choisi de réaliser le même type de configuration pour le système de courriel, pourquoi ?
Soyons honnête avoir soit même par paresse , mais aussi car je n'ai pas de charge sur mon système qui pourrait m'amener à devoir augmenté le nombre de processus requis pour le traitement des spams par exemple. J'ai choisie une voix de simplicité , surtout que je n'ai pas dans la planification à court terme de partagé une partie telle que le service **imap** avec un autre conteneur.

Voici ce que contient mon images de courriel :

* **postfix** : service de __mta__ pour faire l'envoie et gérer la réception de courriel
* **spamd** : Réalise la traitement contre les spams
* **clam** : Service d'antivirus 
* **freshclam** : Service de mise à jour de la base de donnée de virus.
* **postgrey** : Permet de faire du gray listing , afin de réduire les spams ... (bon de vous a moins j'y crois pas mais bon ... :P )
* **opendkim** : Permet de signer les courriels sortant en lien avec les configurations réalisé dans les __DNS__ pour confirmer la provenance.
* **dovecot** : Service de **pop3** et **imap** pour permettre la récupération des courriels par le client
* **ngnix** : Serveur web pour fournir le __webmail__ ainsi que la page d'administration du système
* **SOGo** : Service de webmail
* **admin** : Service d'administration du service

Comme vous pouvez le constater, c'est pas l'image la plus belle mais c'est un tous complet !!

## <a name="ConteneurCentraliser" /> Création du conteneur de BD centraliser.

Il y a par contre un élément manquant dans mon image de gestion de mes courriels, la base de données contenant l'ensemble des informations des utilisateurs ( nom, courriel, alias, mot de passe , calendrier , contacts, ...). 
J'ai opté pour un service de base de données dans un conteneur pour l'ensemble de mes services , car en plus du service de courriels qui utilise une base de données __MySQL__ il y a le service **moodle**, mon **CMS** , mon système de facturation, ... Ils se reposent tous sur une base de donnée , comme je n'avais pas envie d'avoir **un** conteneurs de base de donnée pour chaque service il utilise tous le même pour la production.

Je précise pour la production, car dans le cadre du développement de mes images j'ai utiliser un base de donnée unique pour mes applications .

Voici une représentation graphique de l'utilisation des images et conteneurs :

* **Dev** :

![schema_imgs-conteneur_mysql-dev.png](./imgs/schema_imgs-conteneur_mysql-dev.png)

* **PROD** : 

![schema_imgs-conteneur_mysql-prod.png](./imgs/schema_imgs-conteneur_mysql-prod.png)

Comme vous pouvez le voir j'utilise toujours la même images, donc pas de différence au niveau du système __MySQL__ utilisé que ce soit en développement ou en production. 
Pourquoi cette configuration différente :

* **Dev** :
    * Dans le cadre du développement / validation de la configuration je veux pouvoir détruire ma base de donnée librement pour démarrer avec ma nouvelle image de l'application.
    * Je ne veux pas avoir aucune configuration à réaliser sur la base de donnée, donc quand elle démarrer l'utilisateur et la BD sont créer et dédier pour mon application.
    * Je vous être en mesure de démarrer mon application peut importe le lieu ou je suis portable, ordinateur à la maison sans aucune dépendance autre que mes conteneurs.
    * Si je réalise des stress teste applicatif aucune impacte sur d'autre application en cours.

Bon là vous vous dites pourquoi alors avoir eu une pratique différente pour la production car les raisons nommé ci-dessus s'appliquerait aussi bien en production, sauf peut-être la question de la suppression des données. Réponse ...

* **Prod** :
    * Je ne veux pas a avoir à réaliser des backups sur  4 ou 5 conteneurs de base de données __SQL__ . Je veux avoir un __dump__ complet de mes données dans un fichier . J'ai trop peur d'oublier de prendre une sauvegarde de mes données !!
    * Je n'ai qu'une base de donnée à __Monitorer__ , si nous parlons de performance ou de disponibilité.
    * S'il y a des interactions entre mes applications elles peuvent consulter les données de l'autre système avec le même utilisateur en ajustant les permissions.
    * Je cherche à réduire l'emprunte mémoire et CPU en réduisant le nombre de conteneur (voir section suivante).

Bien entendu ceci est MON choix libre à vous de trouvez cette solution idéal ou non , bien entendu il y a des coté négatif :

* Lors de maintenant l'ensemble des applications sont impacté !

## <a name="noOverHead" /> Docker n'ajoute pas d'overheard, oui MAIS ...

Mon dernier point pour le choix de la mutualisation des bases de données en production indique que je cherche à réduire l'emprunte CPU et mémoire !
C'est étrange, car lors de la présentation général de Docker j'avais spécifier que Docker ne rajoute pas de charge sur le système (__overhead__).

Effectivement Docker ne consomme pas plus de ressource que si l'application était exécuté native , mais si j'ai 5 applications WEB (apache + __MySQL__ ). 

Voici la configuration "classique" sans conteneur : 

![classique_Infra-5web-apps.png](./imgs/classique_Infra-5web-apps.png)

Nous avons donc 1 processus **apache** qui prend l'ensemble des requêtes , en mode __prefork__ par défaut le système démarrer 5 processus pour être en mesure de prendre les requêtes. Lors de la monté en charge il pourra démarrer jusque 150 processus et conservera toujours 10 processus  une fois la monté en charge réalisé.
La base de données aussi fournit le service pour l'ensemble des services, nous avons 1 processus __MySQL__ qui est démarrer pour les 5 applications.

Si nous regardons le mode de fonctionnement avec Docker , nous n'aurons plus 1 processus apache mais 5 , car chaque conteneur étant isolée il devra avoir son propre processus apache pour prendre en charge les requêtes qui arrive. Est-ce grave ?!?! Non pas  pas vraiment , faut juste s'assurer que la configuration de votre apache soit approprié pour votre application . Comme il y avait mutualisation des services sur une machine nous devions permettre à apache de conservé plus de processus "dormant" , alors que pour votre application peut-être que démarrer uniquement 2 processus au startup et en conserver que 4 en cas de monté en charge serait plus adéquat.

Voici la représentation graphique évidente du résultat avec des conteneurs. 

![docker_Infra-5web-apps.png](./imgs/docker_Infra-5web-apps.png)

Personnellement je trouve que ça vaut le coup, car si nous prenons l'économie de __l'overhead__ des __VM__ ce coût est minime.

Je pense que c'est important de le signaler , car il faut le prendre en considération !! 
Vous comprenez pourquoi j'ai mutualisé mon service de base de donnée, maintenant je ne voyais pas de plus values à avoir un processus __MySQL__ par application. L'avenir me donnera peut-être tord , il sera toujours temps de changer ma configuration à ce moment là car c'est la même image :D.

## <a name="UseRegistry" /> Utilisation du registry

Je vous suggère FORTEMENT d'utiliser un __registry__ pour vos images, je ne l'ai pas encore fait j'ai repousser cette opération par manque de temps , cependant sans __registry__ vous êtes obligé de compiler tous le temps vos images. Si vous utilisez un __registry__ ceci veut dire que si vous avez __buildé__ votre image sur une machine elle sera disponible sur les autres via le téléchargement depuis le __registry__ . 
C'est vraiment pénible de devoir recréer l'image sur une nouvelle machine quand vous savez pertinemment que vous l'avez déjà fait , de plus vous pouvez grâce au système de révision (__tag__) des images , vous assurez que vous utiliser toujours la même . Si vous recompilez toujours depuis le __Dockerfile__ , il est possible qu'il y est une modification réaliser dans le __SVN__ ou le GIT résultat vous ne construisez pas l'image à l'identique que ce qui fut testé.

## <a name="ConteneurCommité" /> Création d'un conteneur avec une image "moins belle"

TODO : a valider si je le fait !

# <a name="orchestration" /> Orchestration des conteneurs

Lors de l'introduction, j'ai démarrer mes conteneurs avec la commande **docker run** en passant le nom du conteneur , l'image de référence ainsi que les variables d'environnement utilisé par le conteneur. Ceci est la technique pour démarrer rapidement un conteneur, par contre ceci rencontre RAPIDEMENT la limitation car il faut mémoriser la ligne de commande avec l'ensemble des arguments. 
Quand vous avez un grand nombre de conteneur avec beaucoup de variable vous préférerez avoir un fichier conteneur l'ensemble de l'orchestration pour un service. Heureusement la communauté à pensé à tous , vous avez [docker-compose](https://docs.docker.com/compose/) disponible pour vous aider.

Voici un exemple d'un fichier docker-compose :

        version: '2'
        services:
            x3-cmsms:
                image: x3-cmsms
                container_name : 'x3-cmsms-l'
                hostname: website.x3rus.com
                environment:
                    - DB_HOST=mysql
                    - DB_DATABASE=cms
                    - DB_USERNAME=cms_user
                    - DB_PASSWORD=superPassWord
                    - TIMEZONE=America/Montreal
                    - TERM=xterm
                links:
                    - x3-cmsms-my:mysql
                port:
                    - 80:80
                volumes:
                    - /srv/docker/cms/upload:/var/www/html/cms/upload
            x3-cmsms-my:
                image: mysql:5.5
                container_name : 'x3-cmsms-my-l'
                hostname: cms-mysql.x3rus.com
                environment:
                    - MYSQL_ROOT_PASSWORD=superPatate
                    - TIMEZONE=America/Montreal
                    - TERM=xterm

Prenons une petite minute pour analyser ce fichier :

* **x3-cmsms** et **x3-cmsms-my** : ceci est les 2 services géré par le __docker-compose__ respectivement le service **CMS** et la base de donnée en __backend__.
* **x3-cmsms** :
    * Nous utilisons l'image **x3-cmsms** , nous lui assignons un nom pour le conteneur **x3-cmsms-l** et un hostname **website.x3rus.com**
    * Une liste de variables d'environnement serons assigné lors de l'exécution , principalement les paramètres d'authentification pour la BD ainsi que le fuseau horaire.
    * Bien entendu la service à besoin d'une base de donnée qui est définie dans le même fichier nous réalisons un lien entre les 2 conteneur **links**
    * Suivie de l'assignation de port et répertoire.
* **x3-cmsms-my** :
    * nous utilisons l'image officiel de __MySQL__ soit 5.5 
    * Assignation d'un nom et un nom de machine
    * encore une fois nous retrouvons les variables principalement le mot de passe __root__.

Comme vous pouvez le voir ceci est mon environnement de développement, car mon service de base de donnée est directement lié à l'application en production la dynamique est différente. Je n'utilise pas un **links** mais **external-links** car ma définition de base de donnée n'est pas écrite dans le fichier __docker-compose__ de l'applicatif.

En production j'ai aussi un paramètre indiquant que je veux que le service démarrer toujours après un redémarrage : **restart: always**

## <a name="ChangeVarVol" /> Organisation d'un changement de variable ou volume

Rappelons nous un point important que j'ai mentionné lors de la présentation précédente , lors du démarrage d'un conteneur nous lui assignons des variables lors de l'utilisation de l'instruction **docker run** par la suit nous utilisons **docker start** car nous ne pouvons plus changer les variables d'environnement ou les volumes  !!! 

Résultat regardons ma configuration pour **gitlab**  :


        cm-gitlab:
            image: 'gitlab/gitlab-ce:latest'
            restart: always
            container_name : 'cm-gitlab'
            hostname: source.x3rus.com
            environment:
                TZ: America/Montreal
                GITLAB_OMNIBUS_CONFIG: |
                    external_url 'https://source.x3rus.com'
                    gitlab_rails['time_zone'] = 'America/Montreal'
                    gitlab_rails['gitlab_email_from'] = 'noreply@x3rus.com'
                    gitlab_rails['manage_backup_path'] = true
                    gitlab_rails['backup_path'] = "/var/opt/gitlab/backups"
                    gitlab_rails['backup_archive_permissions'] = 0640
                    gitlab_rails['backup_keep_time'] = 604800
                    gitlab_rails['smtp_enable'] = true
                    gitlab_rails['smtp_address'] = "cm-mail"
                    gitlab_rails['smtp_port'] = 25
                    gitlab_rails['smtp_enable_starttls_auto'] = false
                    gitlab_rails['smtp_tls'] = false
                    pages_nginx['redirect_http_to_https_port'] = 80
                    registry['enable'] = true
                    registry_external_url 'https://rg.source.x3rus.com:8443'
                    gitlab_rails['registry_enabled'] = true
                    gitlab_rails['registry_host'] = "rg.source.x3rus.com"
            external_links:
                - /cm-mailrelay
            ports:
                - '192.99.82.212:22:22'
                - '192.99.82.212:80:80'
                - '192.99.82.212:443:443'
                - '192.99.82.212:8443:8443'
            volumes:
                - '/srv/source/vols/cm-gitlab/gitlab/etc:/etc/gitlab'
                - '/srv/source/vols/cm-gitlab/gitlab/logs:/var/log/gitlab'
                - '/srv/source/vols/cm-gitlab/gitlab/data/:/var/opt/gitlab'

Un des premiers service que j'ai mis en place est le service de **GIT**, évidement lors du démarrage initial j'ai pas mis toutes les configurations possible par manque de connaissance. Si nous regardons les variables ci-dessus nous voyons une liste significative disponible. 

Voici la situation à laquelle je fut confronté , j'avais mon service en place avec l'ensemble de dépôt dans mon serveur git. Je voulais activer la fonction de docker __registry__ dans __gitlab__ grâce au 4 variables suivante :

        registry['enable'] = true
        registry_external_url 'https://rg.source.x3rus.com:8443'
        gitlab_rails['registry_enabled'] = true
        gitlab_rails['registry_host'] = "rg.source.x3rus.com"

Mais pour que ces variables soit prisent en considération, il faut que je détruise mon conteneur actuellement en fonction et le recréer avec les nouvelles variables. Vous comprenez que j'ai commencé à transpirer un peu :P, j'ai fait l'ensemble de mes backups requis et je me suis dis quelle bordel :P. 

Finalement le conteneur de __gitlab__ est bien fait je n'ai rien perdu, je n'ai pas eu non plus a réimporter mes dépôts , car j'avais eu la présence d'esprit de bien suivre la documentation de __gitlab__ et que j'avais créer mes volumes adéquatement. Avec les données en dehors du conteneur le système fut en mesure de retomber sur ces pattes malgré la destruction du conteneur, incluant la Base de donnée __Postgres__ !!

Mais l'avertissement reste valide surtout pour VOS images !!

Vous devez être en mesure de configurer vos conteneurs afin qu'il soit possible de les supprimer et repartir avec d'autre variables, les données doivent donc être à l'extérieur du conteneur (volumes ou base de donnée EXTERNE). 

## <a name="FileStructure" /> Organisation des fichiers et volumes

Voici une suggestion d'organisation de structure que je vous offre, pour le moment ça fait le boulot pour moi, quand il y aura un problème j'ajusterai ...

J'ai structuré comme suit mes fichiers :

* /srv/dockers/
    * Dockerfiles/[Nom_images]/__Dockerfiles__ , __docker-compose__, ... : Ce répertoire contient pour une images donnée mes __dockerfiles__ ainsi que mes __docker-compose__. J'utilise le nom de l'image car une images peut démarrer plusieurs conteneur de même type. Cependant je veux un point de référence sur leur construction et surtout un lieu centralisé pour le démarrage incluant paramètres ...
    * Volumes/[Nom_conteneurs]/répertoires  : Ce répertoire contient les fichiers définie en volume par nom de conteneur me permettant d'avoir une structure intelligible quand je cherche quelque chose.

Avec la commande **docker ps** qui me retourne le nom de l'image et du conteneur en exécution je suis en mesure de __scripter__ des opérations autant sur l'image que sur le conteneur. 

# <a name="NetworkStructure" /> Mise en place de réseau dédié à docker

La structure de réseau présenté lors de la session dernière faisait mention de l'utilisation du réseau __Bridge__ par défaut qui vient avec le service docker.
C'est super bien d'avoir le réseau 172.17.0.0/16, mais voilà j'aime bien le réseau et la segmentation !! De plus je ne suis pas tous seul sur la machine je la partage avec un collègue , bien qu'il est possibilité d'avoir accès au conteneur en fonction, je voulais que les applications soit segmenté !!

J'ai donc utiliser l'instruction **docker network** pour créer d'autre réseau et ainsi avoir :

* Un réseau pour des application commun au 2 __sysadmins__.
* Un réseau pour mon collègue. 
* Un réseau pour moi.
* Un réseau "insécure" , pour les applications que je désire ISOLER complètement .

Ceci à générer un problème : **il n'est pas possible de linké des conteneurs dans 2 réseaux différents** .
C'est une limitation du système :-/, donc certain conteneur commun purent être utilisé avec l'option **links** du conteneur. Le gros problème est qu'il n'y a PAS d'erreur au démarrage, ça ne fonctionne simplement pas ... Ennuyant !! 
La solution à ce problème est d'utiliser l'adresse IP du conteneur, car au niveau réseau pas de problème par défaut , il est possible à une machine d'un réseau 172.18.0.5/24 de communiquer avec le réseau 172.19.0.10/24 !

## <a name="Redirect_port" /> Redirection des ports

En dehors de la mise en place de réseau dédier vous allez vouloir que vos conteneur soit disponible depuis l'extérieur, que ce soit du docker __host__ ou d'Internet directement. Je fut confronté à une réalité que je n'avais pas pensé lors de mes débuts avec __docker__ rien de dramatique, mais je vais le partager avec vous !

Petite précision sur le contexte mon serveur à des règles de __Firewall__ en entré ET en sortie ! C'est règle de __firewall__ sont définie et géré par __puppet__ afin de m'assurer qu'elles sont toujours intègre !!! En d'autre mot si une règle est ajouter manuellement au maximum 30 minutes après elle sera supprimé, car non définie dans __puppet__. 
On est jamais trop prudent :P !!

### <a name="Redirect_port_srcip" /> Détection des ip d'origine

Lors de la mise en place de docker, j'étais partie du principe que j'utiliserai le système de __docker-proxy__ pour faire la redirection des ports vers le conteneur. N'ayant pas un gros trafic l'utilisation d'un processus pour faire le travaille me semblait une bonne idée. De plus telle que mentionné plus tôt comme mes règles sont géré par __puppet__ c'était plus simple. 
Je n'ouvre que la communication pour l'IP externe puis le travail en arrière ce fait tous seul.

Ça fonctionne très bien, MAIS lors de l'analyse de mes logs du serveur web, j'ai constater une chose étrange , l'adresse IP de provenance été TOUJOURS l'IP de la passerelle de docker soit 172.17.0.1. Pour mes statistiques d'utilisation ou pour mettre en place des restrictions par adresse IP c'est une catastrophe !! 
J'ai donc du me rendre à l'évidence que je devrais mettre en place l'autre solution l'utilisation de la redirection de port (**dnat**) avec __iptables__ afin d'être en mesure d'avoir l'adresse IP source. Pour le protocole __http__ il est probable d'utiliser l'en-tête **X-Forwarded-For** , bien que je ne l'ai pas essayer je voulais avoir la connexion direct au moins pour les autres applications qui n'utilise pas le protocole __http__ par exemple __SMTP__ !

### <a name="Redictect_port_Http" /> Conteneur http utilisation des port 80/443

Autre problématique, mais ça je l'avais anticipé, l'utilisation des ports , si j'ai 3 conteneurs pour le service web , donc par exemple :

* [www.x3rus.com](http://www.x3rus.com)
* [moodle.x3rus.com](http://moodle.x3rus.com)
* [mail.x3rus.com](http://mail.x3rus.com)

C'est 3 conteneurs veulent utiliser le port 80 (au minimum) et éventuellement le port 443 , comment je vais 1 processus à la fois peut prendre 1 port sur la machine.

* Option 1 : Utiliser 3 adresses IP externe , heu ... __Ouin__ on est pas encore en __IPv6__ pas convaincu __qu'OVH__ dans mon cas sera d'accord avec l'idée
* Option 2 : Mettre un __proxy__ en avant qui réalise la redirection  , en voilà une bonne idée :D.

J'ai donc mis en place un __proxy__ avec Apache, bon là on peut me lancer des tomates autant que vous voulez disant que c'est pas optimal que apache est trop lourd pour ce type de service! Exacte je suis complètement d'accord avec mes détracteurs , mais attention je le rappel j'ai pas beaucoup de trafic le moment venu j'analyserai d'autre option. Ce que j'ai voulu en utilisant apache en avant plan, c'est  le couteau suisse que ceci m'offre. Apache offre PLEIN de possibilité module pour manipuler les requêtes __http__ et __httpS__. 

Donc voici voici une petit représentation du résultat :

![haproxy-ssl-offload.jpg](./imgs/haproxy-ssl-offload.jpg)


Je peux avoir maintenant un grand nombre de service qui sont fournit par le proxy avec comme critère :

* Un nom de domaine : __www.x3rus.com__ ou __moodle.x3rus.com__
* Un répertoire pour un sous domaine : __www.x3rus.com/cms__

L'image indique que c'est du __offload__ soit du déchargement __SSL__ en fait plus ou moins, car étant sur la même machine la charge du déchiffrement reste présent sur le système cependant pour le diagnostique des problèmes nous n'avons qu'un point de validation. Il est aussi possible d'avoir 1 seul certificat de type **wildcar** donc __\*.x3rus.com__ dans mon cas !

Bien entendu pour le service __httpS__ c'est facile dans le cas de protocole __TCP__ brute vous devrez voir pour un service approprié , telle que mentionné apache étant un couteau suisse je n'ai pas le problème ... Après libre à vous de sélectionné le bon système pour faire la redirection des flux selon vos critères.

### <a name="ChallangeDynIP" /> Le défis des adresse IP dynamique

Le défis des adresses IP fournit en __DHCP__ par docker , en fait j'ai un problème actuellement en cours de résolution , telle que mentionné plus tôt les règles de __firewall__ étant  définie dans __puppet__ je dois inscrire la règle de redirection. Voici un exemple de règles :

        IP externe 192.99.13.211:80 --> ip conteneur 172.17.0.10:80 (nom conteneur  proxy-http)

Parfait ça marche à merveille !! MAIS quand je reboot je n'ai pas de garantie que l'adresse IP du conteneur __proxy-http__ soit la même ceci dépend de la séquence de démarrage des conteneurs que je ne contrôle pas ! Résultat au premier redémarrage surprise plus rien ne fonctionne , transpiration et recherche pourquoi j'ai le problème !

Voici quelque piste de solution.

* Option 1 : assignation d'adresse IP fixe au conteneur lors du démarrage

**Docker-compose** nous offre la possibilité de définir des adresses IP qui seront assigné au conteneur, [voir documentation](https://docs.docker.com/compose/compose-file/#/ipv4address-ipv6address) . Voici un exemple de __docker-compose__ incluant l'assignation d'une adresse IP :

        version: '2'
        services:
          app:
            image: busybox
            command: ifconfig
            networks:
              app_net:
                ipv4_address: 172.16.238.10
                ipv6_address: 2001:3984:3989::10
        networks:
          app_net:
            driver: bridge
            driver_opts:
              com.docker.network.enable_ipv6: "true"
            ipam:
              driver: default
              config:
              - subnet: 172.16.238.0/24
                gateway: 172.16.238.1
              - subnet: 2001:3984:3989::/64
                gateway: 2001:3984:3989::1


Ceci fut testé et fonctionne parfaitement , MAIS je ne privilégie pas cette option , principalement parce que pour être en mesure d'assigner l'adresse IP d'un réseau , vous devez définir le réseau DANS le conteneur . Pour le moment j'ai opté d'avoir 1 __docker-compose__ par service et non avoir l'ensemble dans un GROS fichier __docker-compose__ . En toute honnêteté je ne sais pas si ceci est le bon choix , je me questionne beaucoup sur ce choix , voici mes motivations pour ce choix:

* Je désire avoir 1 fichier de configuration par service afin d'avoir un point de référence de configuration qui peut être utiliser peu importe le lieu (serveur, laptop, ...)
* Je désire avoir un fichier que je peux scripté pour automatisé ou collecté de l'information (argument plus ou moins valide car avec YAML ça ce parce bien, mais bon ... )
* J'aime pas l'idée d'avoir le network définie dans le docker compose , car il est possible que j'utilise un autre réseau selon le lieu de déploiement ...
* Option limitative à la mise en place manuel de la configuration dans le fichier __docker-compose__ rien de dynamique.

Ceci m'a donc a fait en sorte que j'ai analysé une autre option que voici .

* Option 2 : Mise en place d'un __DNS__ dynamique pour Docker

Voici l'option que j'analyse et commence à mettre en place malheureusement par manque de temps je n'ai pas encore de démonstration à offrir. 
Je vais mettre en place un serveur __DNS__ avec __bind9__ ceci gérera plusieurs nom de domaine et offrira un support pour __.dck.x3rus.com__ ce sous domaine permettra la mise à jour des entrés __DNS__ !

Le script **dockerDDNS.py** disponible sur le github :  [https://github.com/ggtools/docker-tools](https://github.com/ggtools/docker-tools) ,
offre la possibilité de prendre en charge les __docker event__ lors de démarrage de conteneur et de mettre à jour les entrés __DNS__ , ceci à l'avantage d'offrir une solution qui **scale** peut importe le réseau une adresse IP sera associé à un nom __DNS__.

Ceci règle en plus la problématique pour les règles de __firewall__ mais aussi permettra de communiquer avec les conteneurs directement peut importe la séquence de démarrage. Ceci fonctionnera peut importe le réseau où évolue le conteneur et me permet de resté paresseux à souhait :D et laisser la machine bossé pour moi :D .

Bon mais ça règle pas ton problème de __firewall__ géré par __puppet__ en fait OUI, car dans la définition de ma règle je vais la définir comme suit :


        IP externe 192.99.13.211:80 --> ip conteneur proxy-http.dck.x3rus.com:80 (nom conteneur  proxy-http ,IP 172.17.0.45 )

Quand puppet va démarrer ce dernier va faire la résolution du nom de domaine et entrer l'IP dans la règle de __firewall__ lors de la prochaine exécution si l'IP à changé __puppet__ dans ça grande intelligence refera le changement .... **MAGIE**

Ceci me permet donc d'avoir nom toujours valide peut importe l'outil qui l'utilisera !! Ceci est bien mieux que la configuration manuelle via le docker compose.

## <a name="ConteneurVPNonly" /> Utilisation de conteneur exclusif au VPN

Voici un point intéressant ne couvrant pas un problème mais plutôt un mode d'utilisation très COOL ! 
J'ai quelques conteneur en fonction sur mon serveur, la majorité son des services "publique" donc directement disponible sur internet telle que le site de formation [http://moodle.x3rus.com](http://moodle.x3rus.com) ou "semi-publique" toujours disponible sur internet mais qui oblige une authentification telle que mon service de courriel / agenda / contact.

Par contre j'ai aussi des conteneurs privé, qui ne sont pas obligatoirement disponible sur internet , par exemple un système de facturation ou de gestion de document . Il y a des informations "sensible" c'est un bien grand mot, mais bon lors utilisation est sporadique et toujours depuis un ordinateur portable ou mon ordinateur personnel . 

Originalement quand je voulais accéder à ces services j'utilisai ma connexion **ssh** avec une redirection de port, ça marche très bien mais c'est pénible obligé d'établir N connexions ssh pour chaque services ... 

J'ai donc mis en place un service de **VPN** avec __openvpn__ sur mon serveur , je suis donc en mesure d'établir une connexion sur mon serveur en toute sécurité , ceci me permet en autre de réaliser mes backups. J'ai réalisé la configuration afin  que la **route** soit poussée lors de l'établissement de connexion. 
Ceci me permet donc d'avoir accès directement au conteneur sans publié sur internet les ports ... Bon un petit graphique pour représenter le tous.

![vpn_route.png](./imgs/vpn_route.png)

Comme vous pouvez le voir lors de l'établissement de connexion la route est poussé résultat quand je met dans mon fureteur : http://172.19.0.10 , ceci me connecte directement sur mon conteneur privé grâce au __VPN__.

Je me retrouve avec un accès à un service privé avec un 2 facteurs authentification grâce au VPN + une authentification sur l'application, NON NON c'est pas **over KILL** :P.

Bon maintenant reparlons du __DNS__ dynamique mentionné avant , encore une fois on voit l'utilité :D :D.


# <a name="ProbHDFull" /> Problème lors d'un disque dur remplie

Parlons maintenant d'un problème MAJEUR !! Lors de la phase de teste avec __docker__ nous avions réaliser nos preuve de concept sur des petites machines avec peu de disque dur. Résultat et vous aurez le même problème que nous vous allez remplir le disque dur , si vous avez assez d'espace vous ne rencontrerez peut-être pas tous de suite le problème . Mais je vous le dis ce ne sera que PIRE !! 

Premièrement vous aurez un nombre élevé de **none** lors de la visualisation des images :

        $ docker images 
        [ ... OUTPUT COUPÉ ... ]
        <none>                        <none>              82644215b784        9 weeks ago         385.4 MB
        <none>                        <none>              77152151a277        9 weeks ago         368.7 MB
        <none>                        <none>              54572d085b49        9 weeks ago         366.5 MB
        <none>                        <none>              207b7e4658a6        9 weeks ago         366.5 MB
        <none>                        <none>              11228a0daa42        9 weeks ago         129.3 MB
        [ ... OUTPUT COUPÉ ... ]

Ceci est dû au fait que vous avez supprimé des images mais pas les conteneurs qui lui font référence , comme docker assure l'intégrité du système il conserve l'image mais la nomme **none**. Vous aurez aussi probablement des systèmes de fichiers orphelin dans vos conteneurs ... :-/.

**Spotify** on eu aussi le même problème, puis comme ils sont cool ils ont libéré leur solution : [https://github.com/spotify/docker-gc](https://github.com/spotify/docker-gc). Ceci est un __gorbage__ collecteur pour docker, il fera l'ensemble du ménage requis.

Bon là on s'excite pas , on va pas mettre ça en production  directement, car par défaut il est brutale , il supprime TOUS qui n'est pas en exécution c'est CHAUD. Il faut le configurer pour définir des exceptions afin qu'il soit plus "smart" . Validez le dans un environnement de développement , mais ça marche super bien !

Bon maintenant qu'arrive t'il si vous ne faites rien et attendez que le l'espace ce remplie... Comme d'habitude quand on laisse trainer quelque chose c'est PIRE !! 
En fait il est fort probable que vos conteneur qui sont en exécution deviennent complètement inutilisable et que vous soyez obligé de les redéployez depuis l'image d'origine , résultat toutes les données contenu dans le système de fichier du conteneur sera perdu !!
Vous aurez des erreurs du style :

        [ 3089.664084] XFS (dm-4): metadata I/O error: block 0x2c230 ("xfs_buf_iodone_callbacks") error 5 numblks 16
        [ 3089.684777] XFS (dm-5): metadata I/O error: block 0x2c230 ("xfs_buf_iodone_callbacks") error 5 numblks 16
        [ 3089.695953] XFS (dm-4): Unmounting Filesystem
        [ 3089.703478] XFS (dm-4): metadata I/O error: block 0x2c230 ("xfs_buf_iodone_callbacks") error 5 numblks 16
        [ 3089.710627] XFS (dm-5): metadata I/O error: block 0x2c230 ("xfs_buf_iodone_callbacks") error 5 numblks 16
        ...

Personnellement nous n'avons pas réussie à récupérer le conteneur , mais dans notre cas ce n'était pas grave nous sommes repartie de l'image original on s'est juste amusé pour apprendre . Bien entendu on a pas pu s'amuser longtemps !!
Avec la version de docker 1.10 nous sommes même arrivé à une situation où nous avons du désinstaller docker pour être en mesure de remettre le serveur réutilisable :-(. Avec la version 1.12 nous n'avons pas rencontré le même problème , bien que l'on a pas appris de nos erreurs et que le problème est revenu :P !!

Je vous dirais donc faire attention ça peut être la catastrophe , surtout à 2:00 du matin d'analyser ce genre de problème ! Monitoring oblige.

## <a name="SpaceLimitConteneur" /> Limitation d'espace par conteneur

Point important chaque conteneur à une limitation d'espace disponible DANS le conteneur , par défaut cette valeur est de 10G , bien entendu ceci ne comprend pas les données mis dans un volume donc à l'extérieur du conteneur. Il est possible d'augmenter cette valeur, par contre ceci est globale , donc s'appliquera pour TOUS les conteneurs sur le systèmes

Vous retrouverez le paramètre à la page :  [https://docs.docker.com/engine/reference/commandline/dockerd/](https://docs.docker.com/engine/reference/commandline/dockerd/), l'argument est **dm.basesize**.

Ça vous protège un peu  pour le problème de remplir l'espace mais ne vous fiez pas la dessus :P !

# <a name="OptDocker" /> Optimisation de Dockers

Mon expérience est assez limité donc pas de miracle sur les suggestions de solution, je me base plus sur mon expérience d'administrateur système 

## <a name="OptDockerFS" /> Assignation d'un Volume Groupe (LVM)
## <a name="MonitoringDck" /> Monitoring du système Docker


